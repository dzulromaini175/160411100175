{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : M Dzul Romaini AL NIM :160411100175 Matkul: Penambangan dan pencarian Web Crawling Data di web Target: https://thoughtcatalog.com/category/creepy/ untuk menjalankan program di perlukan Python 3.6 dengan lib yang terinstall: 1.request 2.beautifulsoup 3.sqlite 3 4.csv 5.numpy 6.scipy 7.scikit -learn 8.scikit fuzzy 9.nltk (untuk mencari kosakata bahasa inggris) Crawling Data di web mengambil data di dalam web dan mengelompokan data yang telah di crawling dengan metode clustering,crawling digunakan untuk mengambil data berupa text,citra,audio,video,dll. Code Programnya: membuat database sqlite 3 dan dikoneksikan pada sqlite database yang bersama koneksi = sqlite3.connect('One_piece.db') koneksi.execute(''' CREATE TABLE if not exists Onepiece \u200b (judul TEXT NOT NULL, \u200b isi TEXT NOT NULL);''') menentukan link yang akan di crawl dan disimpan di variable src,kemudian crawling next page apa saja yang di ambil,pada web yang akan di ambil kali ini,ketika mengambil judul,paragraf dan isi dalam web. \u200b link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b #soup = soup.encode(\"utf-8\") \u200b judul = soup.find(class_='entry-title').getText() \u200b judul = judul.encode() \u200b isi = soup.find(class_='entry-block-group box-content') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi diambil judul dan isi paragraf lantas di cari tag p koneksi.execute('INSERT INTO Onepiece values (?,?)', (judul, p)); \u200b except: \u200b pass koneksi.commit() tampil = koneksi.execute(\"SELECT * FROM Onepiece\") Code Program artikel = soup.findAll(class_='tcf-article-md-content') koneksi = sqlite3.connect('One_piece.db') koneksi.execute(''' CREATE TABLE if not exists Onepiece \u200b (judul TEXT NOT NULL, \u200b isi TEXT NOT NULL);''') for i in range(len(artikel)): \u200b try: \u200b link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b #soup = soup.encode(\"utf-8\") \u200b judul = soup.find(class_='entry-title').getText() \u200b judul = judul.encode() \u200b isi = soup.find(class_='entry-block-group box-content') \u200b paragraf = isi.findAll('p') \u200b p = '' \u200b for s in paragraf: \u200b p+=str(s.getText().encode(\"utf-8\"))[2:-1] +' ' koneksi.execute( INSERT INTO Onepiece values (?,?) , (judul, p)); except: pass tahapnya dilakukan 3 proses: 1.tahap stopword Removal (menghilangkan bacaan yang sering muncul) 2.Stemming(menggunakan suatu kata menjadi kata dasar) 3.Tokenisasi N-Gram Matrik VSM merupakan proses hitung kemunculan semua kata pada suatu kata Code: koneksi.commit() tampil = koneksi.execute(\"SELECT * FROM Onepiece\") with open ('data_crawler.csv', newline='', mode='w', encoding = 'utf-8')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for i in tampil: employee_writer.writerow(i) TF-IDF berguna untuk mencari banyaknya kata yang muncul pada satu dokumen ,IDf digunakan untuk mengetahui dokumen yang mana saja yang memiliki kata yang sama code tf-idf df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w', encoding = 'utf-8') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) code tahap ke 3 stopword removal (menghilangkan tanda baca yang sering muncul) stemming(menggunakan suatu kata menjadi kata dasar)tokenisasi \"N-gram\"(memecah kalimat perkata) code program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u])**2 \u200b bawah_kanan += (data[k,v] - meanFitur[v])**2 \u200b bawah_kiri = bawah_kiri ** 0.5 \u200b bawah_kanan = bawah_kanan ** 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar=katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar,data clustering: pengelompokkan data menjadi k-kelompok,K-means merupakan salah satu metode pengelompokan data yang berusaha mempartisi data menjadi dua/lebih. Code Program katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya=kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) with open('Anggota_cluster.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b for i in classnya.reshape(-1,1): \u200b employee_writer.writerow(i) with open('Seleksi_Fitur.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow([katadasarBaru.tolist()]) \u200b for i in fiturBaru: \u200b employee_writer.writerow(i) Kesimpulan pada dasarnya dari metode yang dicoba,dan melalui percobaan yang panjang,menggunakan seleksi fitur dan clustering dengan menggunakan Fuzzy K-Means merupakan metode terakurat yang pernah saya gunakan dari pada metode lain dan berbelit belit","title":"Crawling"},{"location":"#tf-idf","text":"df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w', encoding = 'utf-8') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) code tahap ke 3 stopword removal (menghilangkan tanda baca yang sering muncul) stemming(menggunakan suatu kata menjadi kata dasar)tokenisasi \"N-gram\"(memecah kalimat perkata) code program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u])**2 \u200b bawah_kanan += (data[k,v] - meanFitur[v])**2 \u200b bawah_kiri = bawah_kiri ** 0.5 \u200b bawah_kanan = bawah_kanan ** 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar=katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar,data clustering: pengelompokkan data menjadi k-kelompok,K-means merupakan salah satu metode pengelompokan data yang berusaha mempartisi data menjadi dua/lebih. Code Program katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya=kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) with open('Anggota_cluster.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b for i in classnya.reshape(-1,1): \u200b employee_writer.writerow(i) with open('Seleksi_Fitur.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow([katadasarBaru.tolist()]) \u200b for i in fiturBaru: \u200b employee_writer.writerow(i) Kesimpulan pada dasarnya dari metode yang dicoba,dan melalui percobaan yang panjang,menggunakan seleksi fitur dan clustering dengan menggunakan Fuzzy K-Means merupakan metode terakurat yang pernah saya gunakan dari pada metode lain dan berbelit belit","title":"tf-idf"}]}