{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : M Dzul Romaini AL NIM :160411100175 Matkul: Penambangan dan pencarian Web Crawling Data di web Target: https://thoughtcatalog.com/category/creepy/ untuk menjalankan program di perlukan Python 3.6 dengan lib yang terinstall: 1.request 2.beautifulsoup 3.sqlite 3 4.csv 5.numpy 6.scipy 7.scikit -learn 8.scikit fuzzy 9.nltk (untuk mencari kosakata bahasa inggris) Crawling Data di web mengambil data di dalam web dan mengelompokan data yang telah di crawling dengan metode clustering,crawling digunakan untuk mengambil data berupa text,citra,audio,video,dll. Code Programnya: membuat database sqlite 3 dan dikoneksikan pada sqlite database yang bersama koneksi = sqlite3.connect('One_piece.db') koneksi.execute(''' CREATE TABLE if not exists Onepiece \u200b (judul TEXT NOT NULL, \u200b isi TEXT NOT NULL);''') menentukan link yang akan di crawl dan disimpan di variable src,kemudian crawling next page apa saja yang di ambil,pada web yang akan di ambil kali ini,ketika mengambil judul,paragraf dan isi dalam web. \u200b link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b #soup = soup.encode(\"utf-8\") \u200b judul = soup.find(class_='entry-title').getText() \u200b judul = judul.encode() \u200b isi = soup.find(class_='entry-block-group box-content') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi diambil judul dan isi paragraf lantas di cari tag p koneksi.execute('INSERT INTO Onepiece values (?,?)', (judul, p)); \u200b except: \u200b pass koneksi.commit() tampil = koneksi.execute(\"SELECT * FROM Onepiece\") Code Program artikel = soup.findAll(class_='tcf-article-md-content') koneksi = sqlite3.connect('One_piece.db') koneksi.execute(''' CREATE TABLE if not exists Onepiece \u200b (judul TEXT NOT NULL, \u200b isi TEXT NOT NULL);''') for i in range(len(artikel)): \u200b try: \u200b link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b #soup = soup.encode(\"utf-8\") \u200b judul = soup.find(class_='entry-title').getText() \u200b judul = judul.encode() \u200b isi = soup.find(class_='entry-block-group box-content') \u200b paragraf = isi.findAll('p') \u200b p = '' \u200b for s in paragraf: \u200b p+=str(s.getText().encode(\"utf-8\"))[2:-1] +' ' koneksi.execute( INSERT INTO Onepiece values (?,?) , (judul, p)); except: pass tahapnya dilakukan 3 proses: 1.tahap stopword Removal (menghilangkan bacaan yang sering muncul) 2.Stemming(menggunakan suatu kata menjadi kata dasar) 3.Tokenisasi N-Gram Matrik VSM merupakan proses hitung kemunculan semua kata pada suatu kata Code: koneksi.commit() tampil = koneksi.execute(\"SELECT * FROM Onepiece\") with open ('data_crawler.csv', newline='', mode='w', encoding = 'utf-8')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for i in tampil: employee_writer.writerow(i) TF-IDF berguna untuk mencari banyaknya kata yang muncul pada satu dokumen ,IDf digunakan untuk mengetahui dokumen yang mana saja yang memiliki kata yang sama code tf-idf df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w', encoding = 'utf-8') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) code tahap ke 3 stopword removal (menghilangkan tanda baca yang sering muncul) stemming(menggunakan suatu kata menjadi kata dasar)tokenisasi \"N-gram\"(memecah kalimat perkata) code program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u])**2 \u200b bawah_kanan += (data[k,v] - meanFitur[v])**2 \u200b bawah_kiri = bawah_kiri ** 0.5 \u200b bawah_kanan = bawah_kanan ** 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar=katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar,data clustering: pengelompokkan data menjadi k-kelompok,K-means merupakan salah satu metode pengelompokan data yang berusaha mempartisi data menjadi dua/lebih. Code Program katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya=kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) with open('Anggota_cluster.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b for i in classnya.reshape(-1,1): \u200b employee_writer.writerow(i) with open('Seleksi_Fitur.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow([katadasarBaru.tolist()]) \u200b for i in fiturBaru: \u200b employee_writer.writerow(i) Kesimpulan pada dasarnya dari metode yang dicoba,dan melalui percobaan yang panjang,menggunakan seleksi fitur dan clustering dengan menggunakan Fuzzy K-Means merupakan metode terakurat yang pernah saya gunakan dari pada metode lain dan berbelit belit","title":"Crawling"},{"location":"#tf-idf","text":"df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w', encoding = 'utf-8') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) code tahap ke 3 stopword removal (menghilangkan tanda baca yang sering muncul) stemming(menggunakan suatu kata menjadi kata dasar)tokenisasi \"N-gram\"(memecah kalimat perkata) code program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u])**2 \u200b bawah_kanan += (data[k,v] - meanFitur[v])**2 \u200b bawah_kiri = bawah_kiri ** 0.5 \u200b bawah_kanan = bawah_kanan ** 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar=katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar,data clustering: pengelompokkan data menjadi k-kelompok,K-means merupakan salah satu metode pengelompokan data yang berusaha mempartisi data menjadi dua/lebih. Code Program katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya=kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) with open('Anggota_cluster.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b for i in classnya.reshape(-1,1): \u200b employee_writer.writerow(i) with open('Seleksi_Fitur.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow([katadasarBaru.tolist()]) \u200b for i in fiturBaru: \u200b employee_writer.writerow(i) Kesimpulan pada dasarnya dari metode yang dicoba,dan melalui percobaan yang panjang,menggunakan seleksi fitur dan clustering dengan menggunakan Fuzzy K-Means merupakan metode terakurat yang pernah saya gunakan dari pada metode lain dan berbelit belit","title":"tf-idf"},{"location":"Web_Structure/","text":"Web Structure Mining Web Structure Mining merupakan salah satu implementasi dari Data Mining. Berbeda dengan Web Content Mining yang berfokus pada isi/konten dari suatu website, Web Structure Mining berfokus pada struktur link pada hypertext antar-dokumen. Secara sederhana, kegiatan Structure Mining merupakan proses mengambil pola hubungan suatu website dengan website lain. Media sosial seperti Facebook, misalnya. Pengguna facebook memiliki hubungan \"pertemanan\" dengan pengguna facebook lain. Jika digambarkan, pola hubungan tersebut akan membentuk sebuah Graph yang sangat besar. Umumnya, Web Structure Mining berfungsi untuk menemukan hubungan antara suatu webpage dengan webpage lain. Hubungan tersebut dapat menentukan apakah kedua webpage tersebut memiliki kemiripan, baik secara struktur maupun konten. Keduanya mungkin saja berada di satu web server yang dibuat oleh satu orang yang sama. Tujuan Salah satu tujuan dari web structure mining adalah mengkategorikan website menurut rangkuman struktur link yang telah didapat berdasarkan kemiripan dan seberapa banyak website itu diakses oleh website yang lain. Tools dan Library yang digunakan Python 3.6 Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Menampung dan Menghitung Pagerank : pa , networkx Library untuk Menampilkan Graph : matplotlib Website target : \" https://thoughtcatalog.com/category/creepy/ \" Code 1 def getAllLinks(src): try: page = requests.get(src)# Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, html.parser ) # GET all tag lt;a gt; tags = soup.findAll( a ) links = [] for tag in tags: # Pencegahan eror apabila link tidak memiliki href try: # Get all link link = tag[ href ] if not link in links and http in link: links.append(link) except KeyError: pass return links except: #print( Error 404 : Page +src+ not found ) return list() code diatas berfungsi untuk mendapatkan semua link pada sebuah website. Yang pertama, kita mendapatkan semua tag a dengan method .findAll(\"a\") . Lalu kita bisa menggunakan tag[\"href\"] untuk mendapatkan link. Code 2 def crawl(url, max_deep, show=False, deep=0, done=[]): #returnnya ada di edgelist global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) #menampilkan proses if not url in done: # crawl semua link links = getAllLinks(url) done.append(url) if show: if deep == 1: print(url) else: print( | , end= ) for i in range(deep-1): print( -- , end= ) print( (%d)%s %(len(links),url)) for link in links: # Membentuk format jalan (edge = gt; (dari, ke)) link = simplifiedURL(link) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep, done) Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = https://thoughtcatalog.com/category/creepy/ edgelist = [] nodelist = [root] crawl(root, 3, show=True) edgeListFrame = pd.DataFrame(edgelist, None, ( From , To )) Graph g = nx.from_pandas_edgelist(edgeListFrame, From , To , None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color= w ) plt.axis( off ) plt.show()","title":"Crawling link"},{"location":"Web_Structure/#web-structure-mining","text":"Web Structure Mining merupakan salah satu implementasi dari Data Mining. Berbeda dengan Web Content Mining yang berfokus pada isi/konten dari suatu website, Web Structure Mining berfokus pada struktur link pada hypertext antar-dokumen. Secara sederhana, kegiatan Structure Mining merupakan proses mengambil pola hubungan suatu website dengan website lain. Media sosial seperti Facebook, misalnya. Pengguna facebook memiliki hubungan \"pertemanan\" dengan pengguna facebook lain. Jika digambarkan, pola hubungan tersebut akan membentuk sebuah Graph yang sangat besar. Umumnya, Web Structure Mining berfungsi untuk menemukan hubungan antara suatu webpage dengan webpage lain. Hubungan tersebut dapat menentukan apakah kedua webpage tersebut memiliki kemiripan, baik secara struktur maupun konten. Keduanya mungkin saja berada di satu web server yang dibuat oleh satu orang yang sama.","title":"Web Structure Mining"},{"location":"Web_Structure/#tujuan","text":"Salah satu tujuan dari web structure mining adalah mengkategorikan website menurut rangkuman struktur link yang telah didapat berdasarkan kemiripan dan seberapa banyak website itu diakses oleh website yang lain. Tools dan Library yang digunakan Python 3.6 Library untuk Crawling : BeautifulSoup4 , requests Library untuk Preprocessing Text : Sastrawi Library untuk Menampung dan Menghitung Pagerank : pa , networkx Library untuk Menampilkan Graph : matplotlib Website target : \" https://thoughtcatalog.com/category/creepy/ \" Code 1 def getAllLinks(src): try: page = requests.get(src)# Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, html.parser ) # GET all tag lt;a gt; tags = soup.findAll( a ) links = [] for tag in tags: # Pencegahan eror apabila link tidak memiliki href try: # Get all link link = tag[ href ] if not link in links and http in link: links.append(link) except KeyError: pass return links except: #print( Error 404 : Page +src+ not found ) return list() code diatas berfungsi untuk mendapatkan semua link pada sebuah website. Yang pertama, kita mendapatkan semua tag a dengan method .findAll(\"a\") . Lalu kita bisa menggunakan tag[\"href\"] untuk mendapatkan link. Code 2 def crawl(url, max_deep, show=False, deep=0, done=[]): #returnnya ada di edgelist global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) #menampilkan proses if not url in done: # crawl semua link links = getAllLinks(url) done.append(url) if show: if deep == 1: print(url) else: print( | , end= ) for i in range(deep-1): print( -- , end= ) print( (%d)%s %(len(links),url)) for link in links: # Membentuk format jalan (edge = gt; (dari, ke)) link = simplifiedURL(link) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep, done) Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = https://thoughtcatalog.com/category/creepy/ edgelist = [] nodelist = [root] crawl(root, 3, show=True) edgeListFrame = pd.DataFrame(edgelist, None, ( From , To )) Graph g = nx.from_pandas_edgelist(edgeListFrame, From , To , None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color= w ) plt.axis( off ) plt.show()","title":"Tujuan"}]}